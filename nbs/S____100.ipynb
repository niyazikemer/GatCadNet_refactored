{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d87d2-0426-460d-99ff-a15b9ab6741f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f40b014b-a2a0-4cf5-85bf-872ff54a6b64",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7371e77e-fff0-4103-81ed-67ed541587ea",
   "metadata": {},
   "source": [
    "## Datasets (train/val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ec83d-dac3-4755-b39f-c9457988858e",
   "metadata": {},
   "source": [
    "## Gat Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06deaab2-b9c5-477f-8a2e-505e471e5cbf",
   "metadata": {},
   "source": [
    "# TO DO: batch normalization and weight normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8a7e2-e007-406a-8140-e69f730b0c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is  original Gat Layer from PYG\n",
    "\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import glorot, zeros, kaiming_uniform\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, softmax\n",
    "\n",
    "\n",
    "class GATv2Conv(MessagePassing):\n",
    "    r\"\"\"The GATv2 operator from the `\"How Attentive are Graph Attention\n",
    "    Networks?\" <https://arxiv.org/abs/2105.14491>`_ paper, which fixes the\n",
    "    static attention problem of the standard\n",
    "    :class:`~torch_geometric.conv.GATConv` layer.\n",
    "    Since the linear layers in the standard GAT are applied right after each\n",
    "    other, the ranking of attended nodes is unconditioned on the query node.\n",
    "    In contrast, in :class:`GATv2`, every node can attend to any other node.\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\alpha_{i,i}\\mathbf{\\Theta}\\mathbf{x}_{i} +\n",
    "        \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}\\mathbf{\\Theta}\\mathbf{x}_{j},\n",
    "\n",
    "    where the attention coefficients :math:`\\alpha_{i,j}` are computed as\n",
    "\n",
    "    .. math::\n",
    "        \\alpha_{i,j} =\n",
    "        \\frac{\n",
    "        \\exp\\left(\\mathbf{a}^{\\top}\\mathrm{LeakyReLU}\\left(\\mathbf{\\Theta}\n",
    "        [\\mathbf{x}_i \\, \\Vert \\, \\mathbf{x}_j]\n",
    "        \\right)\\right)}\n",
    "        {\\sum_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\n",
    "        \\exp\\left(\\mathbf{a}^{\\top}\\mathrm{LeakyReLU}\\left(\\mathbf{\\Theta}\n",
    "        [\\mathbf{x}_i \\, \\Vert \\, \\mathbf{x}_k]\n",
    "        \\right)\\right)}.\n",
    "\n",
    "    If the graph has multi-dimensional edge features :math:`\\mathbf{e}_{i,j}`,\n",
    "    the attention coefficients :math:`\\alpha_{i,j}` are computed as\n",
    "\n",
    "    .. math::\n",
    "        \\alpha_{i,j} =\n",
    "        \\frac{\n",
    "        \\exp\\left(\\mathbf{a}^{\\top}\\mathrm{LeakyReLU}\\left(\\mathbf{\\Theta}\n",
    "        [\\mathbf{x}_i \\, \\Vert \\, \\mathbf{x}_j \\, \\Vert \\, \\mathbf{e}_{i,j}]\n",
    "        \\right)\\right)}\n",
    "        {\\sum_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\n",
    "        \\exp\\left(\\mathbf{a}^{\\top}\\mathrm{LeakyReLU}\\left(\\mathbf{\\Theta}\n",
    "        [\\mathbf{x}_i \\, \\Vert \\, \\mathbf{x}_k \\, \\Vert \\, \\mathbf{e}_{i,k}]\n",
    "        \\right)\\right)}.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample, or :obj:`-1` to\n",
    "            derive the size from the first input(s) to the forward method.\n",
    "            A tuple corresponds to the sizes of source and target\n",
    "            dimensionalities.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        heads (int, optional): Number of multi-head-attentions.\n",
    "            (default: :obj:`1`)\n",
    "        concat (bool, optional): If set to :obj:`False`, the multi-head\n",
    "            attentions are averaged instead of concatenated.\n",
    "            (default: :obj:`True`)\n",
    "        negative_slope (float, optional): LeakyReLU angle of the negative\n",
    "            slope. (default: :obj:`0.2`)\n",
    "        dropout (float, optional): Dropout probability of the normalized\n",
    "            attention coefficients which exposes each node to a stochastically\n",
    "            sampled neighborhood during training. (default: :obj:`0`)\n",
    "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
    "            self-loops to the input graph. (default: :obj:`True`)\n",
    "        edge_dim (int, optional): Edge feature dimensionality (in case\n",
    "            there are any). (default: :obj:`None`)\n",
    "        fill_value (float or Tensor or str, optional): The way to generate\n",
    "            edge features of self-loops (in case :obj:`edge_dim != None`).\n",
    "            If given as :obj:`float` or :class:`torch.Tensor`, edge features of\n",
    "            self-loops will be directly given by :obj:`fill_value`.\n",
    "            If given as :obj:`str`, edge features of self-loops are computed by\n",
    "            aggregating all features of edges that point to the specific node,\n",
    "            according to a reduce operation. (:obj:`\"add\"`, :obj:`\"mean\"`,\n",
    "            :obj:`\"min\"`, :obj:`\"max\"`, :obj:`\"mul\"`). (default: :obj:`\"mean\"`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        share_weights (bool, optional): If set to :obj:`True`, the same matrix\n",
    "            will be applied to the source and the target node of every edge.\n",
    "            (default: :obj:`False`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "\n",
    "    Shapes:\n",
    "        - **input:**\n",
    "          node features :math:`(|\\mathcal{V}|, F_{in})` or\n",
    "          :math:`((|\\mathcal{V_s}|, F_{s}), (|\\mathcal{V_t}|, F_{t}))`\n",
    "          if bipartite,\n",
    "          edge indices :math:`(2, |\\mathcal{E}|)`,\n",
    "          edge features :math:`(|\\mathcal{E}|, D)` *(optional)*\n",
    "        - **output:** node features :math:`(|\\mathcal{V}|, H * F_{out})` or\n",
    "          :math:`((|\\mathcal{V}_t|, H * F_{out})` if bipartite.\n",
    "          If :obj:`return_attention_weights=True`, then\n",
    "          :math:`((|\\mathcal{V}|, H * F_{out}),\n",
    "          ((2, |\\mathcal{E}|), (|\\mathcal{E}|, H)))`\n",
    "          or :math:`((|\\mathcal{V_t}|, H * F_{out}), ((2, |\\mathcal{E}|),\n",
    "          (|\\mathcal{E}|, H)))` if bipartite\n",
    "    \"\"\"\n",
    "    _alpha: OptTensor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: Union[int, Tuple[int, int]],\n",
    "        out_channels: int,\n",
    "        heads: int = 1,\n",
    "        concat: bool = True,\n",
    "        negative_slope: float = 0.2,\n",
    "        dropout: float = 0.0,\n",
    "        add_self_loops: bool = True,\n",
    "        edge_dim: Optional[int] = None,\n",
    "        fill_value: Union[float, Tensor, str] = 'mean',\n",
    "        bias: bool = True,\n",
    "        share_weights: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.edge_dim = edge_dim\n",
    "        self.fill_value = fill_value\n",
    "        self.share_weights = share_weights\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            self.lin_l = Linear(in_channels, heads * out_channels, bias=bias,\n",
    "                                weight_initializer='kaiming_uniform')\n",
    "            if share_weights:\n",
    "                self.lin_r = self.lin_l\n",
    "            else:\n",
    "                self.lin_r = Linear(in_channels, heads * out_channels,\n",
    "                                    bias=bias, weight_initializer='kaiming_uniform')\n",
    "        else:\n",
    "            self.lin_l = Linear(in_channels[0], heads * out_channels,\n",
    "                                bias=bias, weight_initializer='kaiming_uniform')\n",
    "            if share_weights:\n",
    "                self.lin_r = self.lin_l\n",
    "            else:\n",
    "                self.lin_r = Linear(in_channels[1], heads * out_channels,\n",
    "                                    bias=bias, weight_initializer='kaiming_uniform')\n",
    "\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        if edge_dim is not None:\n",
    "            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False,\n",
    "                                   weight_initializer='kaiming_uniform')\n",
    "        else:\n",
    "            self.lin_edge = None\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self._alpha = None\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "        if self.lin_edge is not None:\n",
    "            self.lin_edge.reset_parameters()\n",
    "        #glorot(self.att)    \n",
    "        kaiming_uniform(self.att, self.in_channels,0.1)\n",
    "        zeros(self.bias)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None,\n",
    "                return_attention_weights: bool = None):\n",
    "        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, NoneType) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, NoneType) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, bool) -> Tuple[Tensor, SparseTensor]  # noqa\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            return_attention_weights (bool, optional): If set to :obj:`True`,\n",
    "                will additionally return the tuple\n",
    "                :obj:`(edge_index, attention_weights)`, holding the computed\n",
    "                attention weights for each edge. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        x_l: OptTensor = None\n",
    "        x_r: OptTensor = None\n",
    "        if isinstance(x, Tensor):\n",
    "            assert x.dim() == 2\n",
    "            x_l = self.lin_l(x).view(-1, H, C)\n",
    "            if self.share_weights:\n",
    "                x_r = x_l\n",
    "            else:\n",
    "                x_r = self.lin_r(x).view(-1, H, C)\n",
    "        else:\n",
    "            x_l, x_r = x[0], x[1]\n",
    "            assert x[0].dim() == 2\n",
    "            x_l = self.lin_l(x_l).view(-1, H, C)\n",
    "            if x_r is not None:\n",
    "                x_r = self.lin_r(x_r).view(-1, H, C)\n",
    "\n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "\n",
    "        if self.add_self_loops:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                num_nodes = x_l.size(0)\n",
    "                if x_r is not None:\n",
    "                    num_nodes = min(num_nodes, x_r.size(0))\n",
    "                edge_index, edge_attr = remove_self_loops(\n",
    "                    edge_index, edge_attr)\n",
    "                edge_index, edge_attr = add_self_loops(\n",
    "                    edge_index, edge_attr, fill_value=self.fill_value,\n",
    "                    num_nodes=num_nodes)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                if self.edge_dim is None:\n",
    "                    edge_index = set_diag(edge_index)\n",
    "                else:\n",
    "                    raise NotImplementedError(\n",
    "                        \"The usage of 'edge_attr' and 'add_self_loops' \"\n",
    "                        \"simultaneously is currently not yet supported for \"\n",
    "                        \"'edge_index' in a 'SparseTensor' form\")\n",
    "\n",
    "        # propagate_type: (x: PairTensor, edge_attr: OptTensor)\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), edge_attr=edge_attr,\n",
    "                             size=None)\n",
    "\n",
    "        alpha = self._alpha\n",
    "        self._alpha = None\n",
    "\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            assert alpha is not None\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                return out, (edge_index, alpha)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                return out, edge_index.set_value(alpha, layout='coo')\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor, x_i: Tensor, edge_attr: OptTensor,\n",
    "                index: Tensor, ptr: OptTensor,\n",
    "                size_i: Optional[int]) -> Tensor:\n",
    "        x = x_i + x_j\n",
    "        #Niyazi\n",
    "        #print('called')\n",
    "        if edge_attr is not None:\n",
    "            if edge_attr.dim() == 1:\n",
    "                edge_attr = edge_attr.view(-1, 1)\n",
    "            assert self.lin_edge is not None\n",
    "            edge_attr = self.lin_edge(edge_attr)\n",
    "            edge_attr = edge_attr.view(-1, self.heads, self.out_channels)\n",
    "            x = x + edge_attr\n",
    "\n",
    "        x = F.leaky_relu(x, self.negative_slope)\n",
    "        alpha = (x * self.att).sum(dim=-1)\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        self._alpha = alpha\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        mes = x_j * alpha.unsqueeze(-1)\n",
    "        #Niyazi\n",
    "        #print(mes.shape)\n",
    "        return mes\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, heads={self.heads})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3a976-07de-4269-b937-795a31e64502",
   "metadata": {},
   "source": [
    "## Gat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6badcf84-9982-4a20-a3b8-d0d6cab061ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1314b-42f3-46be-9308-cdb1c780be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "class GAT(torch.nn.Module):\n",
    "    \"\"\"Graph Attention Network\"\"\"\n",
    "    def __init__(self, dim_in, dim_h, dim_out, heads=2):\n",
    "        super().__init__()        \n",
    "        self.gat1 = GATv2Conv(dim_in, dim_h, edge_dim=6, heads=7,add_self_loops=False)\n",
    "        self.gat2 = GATv2Conv(dim_h*heads, dim_h, edge_dim=6, heads=7,add_self_loops=False)\n",
    "        self.gat3 = GATv2Conv(dim_h*heads, dim_h, edge_dim=6, heads=7,add_self_loops=False)\n",
    "        \n",
    "        \n",
    "        self.node_lin_1 = Linear(dim_h*7, 252) \n",
    "        \n",
    "        self.node_lin_2 = Linear(252, 128)\n",
    "         \n",
    "        self.node_lin_3 = Linear(128, 64)\n",
    "         \n",
    "        self.node_lin_4 = Linear(64, dim_out)\n",
    "        \n",
    "        self.ats_1 = Linear(729, 256)\n",
    "        self.ats_2 = Linear(256,216)\n",
    "        self.ats_3 = Linear(216,128)\n",
    "        self.ats_4 = Linear(128,64)\n",
    "        self.ats_5 = Linear(64,32)\n",
    "        self.ats_6 = Linear(32,16)\n",
    "        self.ats_7 = Linear(16, 2)\n",
    "        \n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                          lr=0.0005,betas=(0.95, 0.999),\n",
    "                                          weight_decay=1e-4)\n",
    "\n",
    "    def forward(self, x, edge_index,edge_attr):\n",
    "        \n",
    "        #h = F.dropout(x, p=0.6, training=self.training)\n",
    "        \n",
    "#         PARAMETERS\n",
    "#             return_attention_weights (bool, optional) – If set to True,\n",
    "#             will additionally return the tuple (edge_index, attention_weights),\n",
    "#             holding the computed attention weights for each edge. (default: None)\n",
    "        \n",
    "        #print(f'edge_index:{edge_index.shape},edge_attr : {edge_attr.shape}') \n",
    "        h = self.gat1(x, edge_index,edge_attr, return_attention_weights=True)\n",
    "        ascore1 =  h[1][1].mean(dim=1).unsqueeze(dim=1)\n",
    "        # h = F.relu(h)\n",
    "        #print(f'h out:{h[0].shape},h edge index shape: {h[1][0].shape}, h attention_score_shape: {h[1][1].shape}')\n",
    "        h=h[0]        \n",
    "        h = F.elu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        \n",
    "        \n",
    "        h = self.gat2(h, edge_index,edge_attr,  return_attention_weights=True)\n",
    "        ascore2 =  h[1][1].mean(dim=1).unsqueeze(dim=1)\n",
    "        # print(f'out:{h[0].shape},edge index shape: {h[1][0].shape}, attention_score_shape: {h[1][1].shape}')\n",
    "        h=h[0]\n",
    "        h = F.elu(h)\n",
    "        # h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        \n",
    "        h = self.gat3(h, edge_index,edge_attr, return_attention_weights=True)\n",
    "        ascore3 =  h[1][1].mean(dim=1).unsqueeze(dim=1)\n",
    "        # print(f'out:{h[0].shape},edge index shape: {h[1][0].shape}, attention_score_shape: {h[1][1].shape}')\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "        \n",
    "        edge_ind = h[1][0]\n",
    "        # arranging edge indexes [2,num_edge] to [num_edge,2]\n",
    "        edge_ind = rearrange(edge_ind, 'h w ->  w h')\n",
    "        \n",
    "        #this is the output from Gat Layers\n",
    "        h = h[0]\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------\n",
    "        # this is extra at this time normally I did not use activation for the last layer.\n",
    "        h_activated = F.elu(h)        \n",
    "        node_embeddings = h_activated\n",
    "        #------------------------------------------------------------------------------\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        \n",
    "        \n",
    "        lin_out = torch.relu(self.node_lin_1(node_embeddings))\n",
    "        lin_out = torch.relu(self.node_lin_2(lin_out))\n",
    "        lin_out = torch.relu(self.node_lin_3(lin_out))\n",
    "        lin_out = self.node_lin_4(lin_out)\n",
    "\n",
    "        \n",
    "\n",
    "        att_score_aggregaded = torch.mean(torch.stack((ascore1 , ascore2 , ascore3),dim=0),dim=0)\n",
    "        #print(f'att_score_aggregaded:{att_score_aggregaded.shape}')\n",
    "        # this is  the concatanation process of (node1,node2,attention score between them)\n",
    "        #first get the pairs of node embeddings\n",
    "        pairs = node_embeddings[edge_ind]\n",
    "        #second put them into a same tensor instead of pairs of two tensors.\n",
    "        pairs = rearrange(pairs, 'b h w -> b (h w)')\n",
    "        # third --> (node1,node2,attention)\n",
    "        edge_emb = torch.cat((pairs,att_score_aggregaded),dim=1)                \n",
    "        #print(f'att score: {att_scores.shape}',f' node embedding: {h[0].shape}',f'edge index: {h[1][0].shape}' )\n",
    "\n",
    "        att_out = torch.relu(self.ats_1(edge_emb))\n",
    "        att_out = torch.relu(self.ats_2(att_out))\n",
    "        att_out = torch.relu(self.ats_3(att_out))\n",
    "        att_out = torch.relu(self.ats_4(att_out))\n",
    "        att_out = torch.relu(self.ats_5(att_out))\n",
    "        att_out = torch.relu(self.ats_6(att_out))\n",
    "        att_out = self.ats_7(att_out)\n",
    "        #print(f'att_out: {att_out[:10]}')  \n",
    "        #h[0] is node embeddings.\n",
    "        \n",
    "        att_out= torch.sigmoid(att_out)\n",
    "        #att_out= F.softmax(att_out)\n",
    "        \n",
    "        \n",
    "        return F.log_softmax(lin_out, dim=1),att_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b48dd7-ecd9-44d4-a097-5e74743d9b95",
   "metadata": {},
   "source": [
    "## Accuracy and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d168fb-2053-4f60-aae1-176c82d853eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, y):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return ((pred_y == y).sum() / len(y)).item()\n",
    "\n",
    "def accuracy2(pred_yy, yy):\n",
    "    \"\"\"Calculate accuracy2.\"\"\"\n",
    "    #this makes the prediction results(same or not) complement to 2\n",
    "    sm = torch.nn.Softmax(dim=1)\n",
    "    soft_pred = sm(pred_yy)\n",
    "    #this makes wrong predictions zero:\n",
    "    # tensor([[0.0000, 0.4014],\n",
    "    #     [0.6602, 0.0000],\n",
    "    #     [0.5958, 0.0000],\n",
    "    #     ...,\n",
    "    #     [0.6334, 0.0000],\n",
    "    #     [0.6778, 0.0000],\n",
    "    #     [0.6734, 0.0000]], device='cuda:0')\n",
    "    out= soft_pred.where(yy == 1 ,yy)\n",
    "    #print(f'out size: {out.size()}')\n",
    "    out = out[:,0] + out[:,1]\n",
    "    #print(f'out size: {out.size()}')\n",
    "    outTrue = (out > .7)\n",
    "    #print(f'out True: {outTrue.size()}')\n",
    "    #print(f'len yy: {len(yy)}')\n",
    "    #Count Trues\n",
    "    #print(f'percent:{(torch.count_nonzero(opTrue) / len(yy)).item()}')\n",
    "    #print(f'nonzero:{torch.count_nonzero(outTrue)}')\n",
    "    return (torch.count_nonzero(outTrue) / len(yy)).item()\n",
    "\n",
    "def train(model,train_dl,valid_dl):\n",
    "    \"\"\"Train a GNN model and return the trained model.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = model.optimizer\n",
    "    epochs = 150\n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        # Training\n",
    "        #print('epoch')\n",
    "        cumul_loss,tot_loss, tot_acc,tot_acc2,tot_loss1,tot_loss2 = 0.,0.,0.,0.,0.,0.\n",
    "        #print(len(train_dl))\n",
    "        \n",
    "        #thi is number of nodes in every batch\n",
    "        #datax=[]\n",
    "        counter = 0.\n",
    "        for data in train_dl:\n",
    "            nt = len(train_dl)\n",
    "            #print(nt)             \n",
    "                        \n",
    "            pairs = rearrange(data.edge_index, 'h w -> w h')\n",
    "            edge_vertex_class = data.y[pairs]\n",
    "            same_class_or_not = edge_vertex_class[:,0] == edge_vertex_class[:,1]\n",
    "            \n",
    "            weight_yy = torch.ones( data.yy.size(),device=device)\n",
    "            \n",
    "            weight_yy[:,0][same_class_or_not] = 10            \n",
    "            criterion2 = torch.nn.BCELoss(weight=weight_yy)\n",
    "            \n",
    "            data = data.to(device)\n",
    "            out1,out2 = model(data.x, data.edge_index,data.edge_attr)\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            loss1 = criterion(out1, data.y)\n",
    "            # print(f'same_class_or_not:{same_class_or_not.size()}')\n",
    "            # print(f'weight_yy:{weight_yy.size()}')\n",
    "            # print(f'out2:{out2.size()}')\n",
    "            loss2 = criterion2(out2, data.yy)\n",
    "            \n",
    "            # print(f'loss2{loss2:.2f}'\n",
    "            #       f'target shape{data.yy[:10]}'\n",
    "            #      )\n",
    "            loss =loss1 + loss2\n",
    "            tot_loss1 += loss1\n",
    "            tot_loss2 += loss2\n",
    "            tot_loss += loss\n",
    "            cumul_loss += loss\n",
    "            acc = accuracy(out1.argmax(dim=1), data.y)\n",
    "            acc2 = accuracy2(out2, data.yy)\n",
    "            tot_acc += acc\n",
    "            tot_acc2 += acc2\n",
    "            # if counter % 5 == 0:\n",
    "            #     loss.backward()\n",
    "            #     optimizer.step()\n",
    "            #     cumul_loss = 0.\n",
    "            #     optimizer.zero_grad()\n",
    "                        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumul_loss = 0.\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        #print(f'number of nodes {max(datax)}')\n",
    "        \n",
    "        \n",
    "        tot_loss = tot_loss/nt\n",
    "        tot_acc = tot_acc/nt\n",
    "        tot_acc2 = tot_acc2/nt\n",
    "        tot_loss1 = tot_loss1/nt\n",
    "        tot_loss2 = tot_loss2/nt\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            tot_v_loss, tot_v_acc,tot_v_acc2,tot_v_loss1,tot_v_loss2 = 0.,0.,0.,0.,0.\n",
    "            #print(len(valid_dl))\n",
    "            for v_data in valid_dl:\n",
    "                # Validation\n",
    "                \n",
    "                v_data=v_data.to(device)\n",
    "                pairs = rearrange(v_data.edge_index, 'h w -> w h')\n",
    "                edge_vertex_class = v_data.y[pairs]\n",
    "                same_class_or_not = edge_vertex_class[:,0] == edge_vertex_class[:,1]\n",
    "            \n",
    "                weight_yy = torch.ones(v_data.yy.size(),device=device)\n",
    "            \n",
    "                weight_yy[:,0][same_class_or_not] = 10            \n",
    "                criterion2 = torch.nn.BCELoss(weight=weight_yy)\n",
    "                v_out1,v_out2 = model(v_data.x, v_data.edge_index,v_data.edge_attr)\n",
    "                \n",
    "                #v_data.yy= torch.abs(torch.cat((data.yy,data.yy-1),dim=1).to(torch.float))\n",
    "                #print(v_data.yy.shape)\n",
    "                tot_v_loss1 += criterion(v_out1, v_data.y)\n",
    "                tot_v_loss2 += criterion2(v_out2, v_data.yy)\n",
    "                \n",
    "                tot_v_acc += accuracy(v_out1.argmax(dim=1), v_data.y)\n",
    "                tot_v_acc2 += accuracy2(v_out2, v_data.yy)\n",
    "                \n",
    "\n",
    "            nv = len(valid_dl)\n",
    "            tot_v_loss1 = tot_v_loss1/nv\n",
    "            tot_v_loss2 = tot_v_loss2/nv\n",
    "            tot_v_acc = tot_v_acc/nv\n",
    "            tot_v_acc2 = tot_v_acc2/nv\n",
    "        # Print metrics every 10 epochs\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f'Epoch {epoch:>3} |\\n  '\n",
    "                  f'Train Loss:  {tot_loss:.2f} | Val Loss:  {tot_v_loss:.2f}\\n'                    \n",
    "                  f'Train Loss1: {tot_loss1:.2f} | Train Loss2: {tot_loss2:.2f}\\n '\n",
    "                  f'Val Loss1: {tot_v_loss1:.2f} | Val Loss2: {tot_v_loss2:.2f}\\n '\n",
    "                  f'Train Acc 1: {tot_acc*100:.2f}% | Train Acc 2: {tot_acc2*100:.2f}%\\n '                  \n",
    "                  f'Val Acc1: {tot_v_acc*100:.2f}% | Val Acc2: {tot_v_acc2*100:.2f}% '\n",
    "                                    \n",
    "                  )\n",
    "          \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d157c3-f777-41c7-9f22-564673f990e1",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8be70c-22f9-4fc3-a2f5-95afb240a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = floorPlanCad(path/\"dataset_train_full/\")\n",
    "#train_ds = floorPlanCad(path/\"_mini_toy_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113aa8db-6f86-4408-90d2-b2ab144f28af",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = floorPlanCad(path/\"dataset_validation_full\")\n",
    "#val_ds = floorPlanCad(path/\"_mini_toy_dataset_validation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97715f0-91a9-4bc2-a9f1-557c76303c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f5778f-a626-4b06-b9b0-063a3e8c896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl = DataLoader(val_ds, batch_size=15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa06866-ad97-4f74-8d17-ee68cd2066be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat = GAT(train_ds.num_features, 52, 36,heads=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37accd1a-c4ed-4a66-901f-2674e24da5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat = gat.to('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c7dd7-753e-428e-816b-bbaa048c646f",
   "metadata": {},
   "source": [
    "only Gat layers initialized with kaiming lineer should be initialized too.\n",
    "\n",
    "models/three_layers_52_hidden_150_ep_full_dataset_84_4__87_6_kaiming.pt\n",
    "\n",
    "three_layers_52_hidden_150_ep_full_dataset_83_6__83_8_glorot\n",
    "```Python\n",
    "GAT(\n",
    "  (gat1): GATv2Conv(4, 52, heads=7)\n",
    "  (gat2): GATv2Conv(364, 52, heads=7)\n",
    "  (gat3): GATv2Conv(364, 52, heads=7)\n",
    "  (node_lin_1): Linear(364, 252, bias=True)\n",
    "  (node_lin_2): Linear(252, 128, bias=True)\n",
    "  (node_lin_3): Linear(128, 64, bias=True)\n",
    "  (node_lin_4): Linear(64, 36, bias=True)\n",
    "  (ats_1): Linear(729, 256, bias=True)\n",
    "  (ats_2): Linear(256, 216, bias=True)\n",
    "  (ats_3): Linear(216, 128, bias=True)\n",
    "  (ats_4): Linear(128, 64, bias=True)\n",
    "  (ats_5): Linear(64, 32, bias=True)\n",
    "  (ats_6): Linear(32, 16, bias=True)\n",
    "  (ats_7): Linear(16, 2, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117dba6b-046e-4835-b67f-3f1a59e61b97",
   "metadata": {},
   "source": [
    "# kaiming_uniform batchsize:15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f2404-26c8-48c0-96de-a22b743104ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 |\n",
      "  Train Loss:  3.46 | Val Loss:  0.00\n",
      "Train Loss1: 2.50 | Train Loss2: 0.96\n",
      " Val Loss1: 2.38 | Val Loss2: 0.92\n",
      " Train Acc 1: 34.22% | Train Acc 2: 32.94%\n",
      " Val Acc1: 32.00% | Val Acc2: 11.66% \n",
      "Epoch  10 |\n",
      "  Train Loss:  1.96 | Val Loss:  0.00\n",
      "Train Loss1: 1.30 | Train Loss2: 0.66\n",
      " Val Loss1: 1.38 | Val Loss2: 0.71\n",
      " Train Acc 1: 62.81% | Train Acc 2: 66.29%\n",
      " Val Acc1: 61.09% | Val Acc2: 63.66% \n",
      "Epoch  20 |\n",
      "  Train Loss:  1.43 | Val Loss:  0.00\n",
      "Train Loss1: 0.89 | Train Loss2: 0.55\n",
      " Val Loss1: 0.91 | Val Loss2: 0.58\n",
      " Train Acc 1: 75.07% | Train Acc 2: 71.30%\n",
      " Val Acc1: 75.27% | Val Acc2: 73.15% \n",
      "Epoch  30 |\n",
      "  Train Loss:  1.16 | Val Loss:  0.00\n",
      "Train Loss1: 0.71 | Train Loss2: 0.45\n",
      " Val Loss1: 0.83 | Val Loss2: 0.53\n",
      " Train Acc 1: 80.04% | Train Acc 2: 76.60%\n",
      " Val Acc1: 77.65% | Val Acc2: 72.75% \n",
      "Epoch  40 |\n",
      "  Train Loss:  1.03 | Val Loss:  0.00\n",
      "Train Loss1: 0.64 | Train Loss2: 0.39\n",
      " Val Loss1: 0.74 | Val Loss2: 0.48\n",
      " Train Acc 1: 82.21% | Train Acc 2: 79.00%\n",
      " Val Acc1: 79.92% | Val Acc2: 78.55% \n",
      "Epoch  50 |\n",
      "  Train Loss:  0.94 | Val Loss:  0.00\n",
      "Train Loss1: 0.58 | Train Loss2: 0.36\n",
      " Val Loss1: 0.71 | Val Loss2: 0.45\n",
      " Train Acc 1: 83.60% | Train Acc 2: 80.81%\n",
      " Val Acc1: 80.85% | Val Acc2: 79.04% \n",
      "Epoch  60 |\n",
      "  Train Loss:  0.87 | Val Loss:  0.00\n",
      "Train Loss1: 0.54 | Train Loss2: 0.34\n",
      " Val Loss1: 0.67 | Val Loss2: 0.44\n",
      " Train Acc 1: 84.83% | Train Acc 2: 82.13%\n",
      " Val Acc1: 82.44% | Val Acc2: 80.98% \n",
      "Epoch  70 |\n",
      "  Train Loss:  0.84 | Val Loss:  0.00\n",
      "Train Loss1: 0.52 | Train Loss2: 0.32\n",
      " Val Loss1: 0.65 | Val Loss2: 0.44\n",
      " Train Acc 1: 85.32% | Train Acc 2: 83.13%\n",
      " Val Acc1: 83.02% | Val Acc2: 79.79% \n",
      "Epoch  80 |\n",
      "  Train Loss:  0.78 | Val Loss:  0.00\n",
      "Train Loss1: 0.50 | Train Loss2: 0.29\n",
      " Val Loss1: 0.65 | Val Loss2: 0.40\n",
      " Train Acc 1: 86.02% | Train Acc 2: 84.53%\n",
      " Val Acc1: 83.41% | Val Acc2: 81.97% \n",
      "Epoch  90 |\n",
      "  Train Loss:  0.75 | Val Loss:  0.00\n",
      "Train Loss1: 0.47 | Train Loss2: 0.28\n",
      " Val Loss1: 0.63 | Val Loss2: 0.41\n",
      " Train Acc 1: 86.74% | Train Acc 2: 85.23%\n",
      " Val Acc1: 83.62% | Val Acc2: 83.34% \n",
      "Epoch 100 |\n",
      "  Train Loss:  0.73 | Val Loss:  0.00\n",
      "Train Loss1: 0.46 | Train Loss2: 0.27\n",
      " Val Loss1: 0.63 | Val Loss2: 0.40\n",
      " Train Acc 1: 86.97% | Train Acc 2: 85.65%\n",
      " Val Acc1: 83.74% | Val Acc2: 81.72% \n",
      "Epoch 110 |\n",
      "  Train Loss:  0.72 | Val Loss:  0.00\n",
      "Train Loss1: 0.45 | Train Loss2: 0.27\n",
      " Val Loss1: 0.61 | Val Loss2: 0.40\n",
      " Train Acc 1: 87.16% | Train Acc 2: 85.72%\n",
      " Val Acc1: 84.71% | Val Acc2: 83.09% \n",
      "Epoch 120 |\n",
      "  Train Loss:  0.69 | Val Loss:  0.00\n",
      "Train Loss1: 0.44 | Train Loss2: 0.25\n",
      " Val Loss1: 0.62 | Val Loss2: 0.39\n",
      " Train Acc 1: 87.62% | Train Acc 2: 86.52%\n",
      " Val Acc1: 84.25% | Val Acc2: 84.76% \n",
      "Epoch 130 |\n",
      "  Train Loss:  0.69 | Val Loss:  0.00\n",
      "Train Loss1: 0.45 | Train Loss2: 0.24\n",
      " Val Loss1: 0.60 | Val Loss2: 0.39\n",
      " Train Acc 1: 87.35% | Train Acc 2: 86.89%\n",
      " Val Acc1: 84.86% | Val Acc2: 84.62% \n",
      "Epoch 140 |\n",
      "  Train Loss:  0.65 | Val Loss:  0.00\n",
      "Train Loss1: 0.42 | Train Loss2: 0.23\n",
      " Val Loss1: 0.60 | Val Loss2: 0.39\n",
      " Train Acc 1: 88.09% | Train Acc 2: 87.61%\n",
      " Val Acc1: 85.06% | Val Acc2: 84.86% \n",
      "Epoch 150 |\n",
      "  Train Loss:  0.64 | Val Loss:  0.00\n",
      "Train Loss1: 0.42 | Train Loss2: 0.23\n",
      " Val Loss1: 0.60 | Val Loss2: 0.38\n",
      " Train Acc 1: 88.12% | Train Acc 2: 87.76%\n",
      " Val Acc1: 84.89% | Val Acc2: 84.48% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GAT(\n",
       "  (gat1): GATv2Conv(4, 52, heads=7)\n",
       "  (gat2): GATv2Conv(364, 52, heads=7)\n",
       "  (gat3): GATv2Conv(364, 52, heads=7)\n",
       "  (node_lin_1): Linear(364, 252, bias=True)\n",
       "  (node_lin_2): Linear(252, 128, bias=True)\n",
       "  (node_lin_3): Linear(128, 64, bias=True)\n",
       "  (node_lin_4): Linear(64, 36, bias=True)\n",
       "  (ats_1): Linear(729, 256, bias=True)\n",
       "  (ats_2): Linear(256, 216, bias=True)\n",
       "  (ats_3): Linear(216, 128, bias=True)\n",
       "  (ats_4): Linear(128, 64, bias=True)\n",
       "  (ats_5): Linear(64, 32, bias=True)\n",
       "  (ats_6): Linear(32, 16, bias=True)\n",
       "  (ats_7): Linear(16, 2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(gat,train_dl,val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795eb1e-4cbb-4922-b318-b67bfa668750",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gat.state_dict(), path/\"models/three_layers_52_hidden_150_ep_full_dataset_84_4__87_6_kaiming.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a51dba-8fe3-48a6-976d-347fd063ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gat.load_state_dict(torch.load(path/\"models/84_72_100_7_layers_att_sigmoid_beta_95_1e-4-full_dataset.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28ed6e-9906-4b3e-9606-b833463eb0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_name = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58fde14-4111-4d16-89e8-364a52ab2fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(36):\n",
    "    num_to_name[str(i+1)]= str(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9745f4c0-7a81-45e4-8e06-fd83db4b3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_name['1'] = 'wall'\n",
    "num_to_name['3'] = 'single door'\n",
    "num_to_name['4'] = 'double door'\n",
    "num_to_name['5'] = 'sliding door'\n",
    "num_to_name['9'] = 'window-9'\n",
    "num_to_name['13'] = 'sofa'\n",
    "num_to_name['14'] = 'bed'\n",
    "num_to_name['15'] = 'chair'\n",
    "num_to_name['16'] = 'table'\n",
    "num_to_name['17'] = 'tv cabinet'\n",
    "num_to_name['18'] = 'wardrobe'\n",
    "num_to_name['19'] = 'cabinet'\n",
    "num_to_name['20'] = 'refrigerator'\n",
    "num_to_name['22'] = 'gas stove'\n",
    "num_to_name['23'] = 'sink'\n",
    "num_to_name['29'] = 'toilet'\n",
    "num_to_name['31'] = 'elevator'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c78c48-1008-443b-b7cf-af9db15e2d95",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8faff9-f25b-4361-a86b-703dfc3bbd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model,v_data):      \n",
    "    with torch.no_grad():\n",
    "        v_data.edge_index,v_data.edge_attr = remove_self_loops(v_data.edge_index,v_data.edge_attr)\n",
    "        model.eval()\n",
    "        v_data=v_data.to('cuda:0')\n",
    "        v_out1,v_out2 = model(v_data.x, v_data.edge_index,v_data.edge_attr)\n",
    "        print(\n",
    "            f'data: {v_data} |\\n |'\n",
    "            f'class_shape: {v_out1.shape} | '\n",
    "            f'instance_shape: {v_out2.shape}')\n",
    "    return v_out1,v_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd8be0-68d7-4f4a-98fa-a54d258283ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[720, 4], edge_index=[2, 10446], edge_attr=[10446, 6], y=[720], yy=[10446, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.load(path/\"_mini_toy_dataset_validation/processed/d_0969-0027.pt\")\n",
    "#data = torch.load(path/\"dataset_validation_full/processed/d_0001-0072.pt\")\n",
    "#data = torch.load(path/\"dataset_validation_full/processed/d_0000-0003.pt\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f3832-6525-4b82-ad4e-5066c9488c54",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "## or dallas presentation\n",
    "lots of manipulation needed to implement the paper, use pdb to explain where and how\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be8afe-8a54-4097-91df-e1f15e40d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1,v2 = infer(gat,data)\n",
    "#v1.shape,v2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1889516-9e54-44cd-9421-1198a06a697e",
   "metadata": {},
   "source": [
    "# Check Accuracy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6812db2a-95af-4d14-acf5-ae00c5822140",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988e91f-81e7-4a84-b4fa-6a5758721346",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d09bec-7316-4ffd-8439-233962e41fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf2011-f0f3-40a2-8f49-82f829d9e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "input = v2\n",
    "output = m(data.yy)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117b35e-f953-4cc4-a6a5-bacdd5dc24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "input = v2\n",
    "output = m(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5fa25c-e35a-4614-a807-050fe68b7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vv = v2[:10,:].where(yy10 == 1 ,yy10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5a36f-9a8c-4b0f-aae6-1dbfdac48376",
   "metadata": {},
   "outputs": [],
   "source": [
    "op= output.where(data.yy == 1 ,data.yy)\n",
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a962b-0d3e-4e54-b647-746812d4b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = op[:,0] + op[:,1]\n",
    "op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544bae87-d3c9-48f6-a589-d012b80a649d",
   "metadata": {},
   "source": [
    "I think it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d8e5fe-dab7-446f-95eb-a93a3b86145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opTrue = (op > .5)\n",
    "\n",
    "print(opTrue)\n",
    "\n",
    "torch.count_nonzero(opTrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47571a7b-6374-42d0-8e3f-2a6f03023c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "9537/12542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef42904-6b71-4e46-8ed4-4b579d090ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation = path/\"dataset_validation_full/raw/0001-0072.svg\"\n",
    "visualisation = path/\"_mini_toy_dataset_validation/raw/0969-0027.svg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ed60e-a77e-4d82-aebe-91e2ce9cafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, attributes = svg2paths(visualisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae38a61-9467-46fc-810d-934282ee2141",
   "metadata": {},
   "source": [
    "## **This part made for checking if the model could predict the real connection between members of a instance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c530c11-3589-43f5-b594-c94378e22e43",
   "metadata": {},
   "source": [
    "#### **Clean attributes and paths from drawing not from the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0324c112-3d70-42dd-9cd7-c44dc97b55e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "atts = []\n",
    "pths = []\n",
    "#available class list in the drawing\n",
    "drw_sem_id_list = []\n",
    "for att in attributes:    \n",
    "    if ('semantic-id' and 'instance-id') in att:\n",
    "        idx = attributes.index(att)\n",
    "        atts.append(attributes[idx])\n",
    "        pths.append(paths[idx])\n",
    "        if int(att['semantic-id']) not in drw_sem_id_list:\n",
    "            drw_sem_id_list.append(int(att['semantic-id']))\n",
    "        \n",
    "len(atts),len(pths), sorted(drw_sem_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d278c5-c4ed-4619-aced-8681f3f4b4fc",
   "metadata": {},
   "source": [
    "## **Vertex Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf160b5f-7f0b-44c2-a74f-3e20e2892517",
   "metadata": {},
   "source": [
    "### **This is how many class in this particular prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f7354-da3c-4299-ae92-9e395cf7713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_sem_id_list = set(v1.argmax(dim=1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787018c4-8bb2-4f6d-9a2b-6199c3aa9b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction_sem_id_list), prediction_sem_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa94e8-7d5a-40ed-a9c6-dc1c042c07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = v1.argmax(dim=1).tolist()\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04774c7-6ca5-4896-bf95-432eeb4ea6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vertex class predictions\n",
    "vertex_dict = {}\n",
    "for i in  prediction_sem_id_list:\n",
    "    vertex_dict[str(i)]= []\n",
    "    \n",
    "for idx, pred in enumerate(preds):\n",
    "    temp_list = vertex_dict[str(pred)]\n",
    "    temp_list.append(idx)\n",
    "    vertex_dict[str(pred)] = temp_list\n",
    "        \n",
    "#print(vertex_dict['15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442b995-ecfc-4501-ae48-55277a7773f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6a15c-a1b5-4563-93dd-e6835e28691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all edges in pairs\n",
    "pairs = rearrange(data.edge_index, 'h w -> w h')\n",
    "len(pairs), pairs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26639d3b-0125-42e5-adb9-00b14b785912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.yy[:10]  same_class_or_not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522a4d8-f169-493c-9281-f1bc475d0eb7",
   "metadata": {},
   "source": [
    "this is for loss weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c097161d-f655-48b5-b94f-906ff2b37197",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_vertex_class = data.y[pairs]\n",
    "same_class_or_not = edge_vertex_class[:,0] == edge_vertex_class[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91301bd-98f9-4743-bcbc-c096005c75e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_vertex_class.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209c3f2-2c27-4a14-bed0-5499d85101ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2,v2.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f8e93-1783-4cb5-bdf6-9cee0c862c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#possibility_tensor_bool = (v2[:,0]>0.8) & (v2[:,1]<0.5)\n",
    "possibility_tensor_bool = (v2[:,0]>v2[:,1]) & (v2[:,1]<0.5)\n",
    "possibility_tensor_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c35e82-39c5-4374-870b-f8a2131c9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs[possibility_tensor_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7989c0-21fe-4b53-af7f-828bb40fb463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs in list format\n",
    "pairs = pairs.tolist()\n",
    "pairs[0],len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8daed86-9993-4ab5-b0b6-ce2903c9cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edge dictionary and idx's \n",
    "edge_dict = {}\n",
    "for i in  prediction_sem_id_list:\n",
    "    edge_dict[str(i)]= []\n",
    "edge_dict_idx = {}\n",
    "for i in  prediction_sem_id_list:\n",
    "    edge_dict_idx[str(i)]= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9566322-089c-4ac0-a5bd-17e7c07f6648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting pairs for each classes\n",
    "for k,v in vertex_dict.items():\n",
    "    for idx,i in  enumerate(pairs):\n",
    "        if (i[0] in v) and (i[1] in v):\n",
    "            edge_dict[k].append(i)\n",
    "            edge_dict_idx[k].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484af728-7490-4cc6-8036-3f955da86aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dict_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692f5fe-d6c0-4071-80b8-44fddab144d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing vertexes that don't have a connection in that class\n",
    "# this can be considered as verification of previous step.\n",
    "# no need to make this if vertex dict not necessary anymore which\n",
    "# is optional. it can be done during the loop\n",
    "for k,v in edge_dict.items():\n",
    "    nodes = set()\n",
    "    nodes = nodes.union(set(item[0] for item in v))\n",
    "    nodes = nodes.union(set(item[1] for item in v))\n",
    "    for i in vertex_dict[k]:\n",
    "        if i not in nodes:\n",
    "            vertex_dict[k].remove(i)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a205ae97-d5dc-490e-8a54-8af2f12dd8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# links = [\n",
    "#     (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (1, 7), (7, 9), (7, 8), (8, 9), # Component 1\n",
    "#     (10, 11), (11, 12), (10, 12) # Component 2\n",
    "# ]\n",
    "\n",
    "# # Create list of all nodes\n",
    "# nodes = set()\n",
    "# nodes = nodes.union(set(item[0] for item in links))\n",
    "# nodes = nodes.union(set(item[1] for item in links))\n",
    "\n",
    "# # Create a data structure in the form of  from -> List<to>\n",
    "# # {\"1\":[2,7],\"2\":[3],\"3\":[4],\"4\":[5],\"5\":[6],\"7\":[9,8],\"8\":[9],\"10\":[11,12],\"11\":[12]}\n",
    "# links_by_nodes = {}\n",
    "# for x, y in links:\n",
    "#     if x in links_by_nodes:\n",
    "#         links_by_nodes[x].append(y)\n",
    "#     else:\n",
    "#         links_by_nodes[x] = [y]\n",
    "\n",
    "def find_connected_components(all_nodes, links_by_nodes):\n",
    "    connected_components = []\n",
    "    all_component_nodes = set()\n",
    "    for node in all_nodes:\n",
    "        if node in all_component_nodes:\n",
    "            continue\n",
    "        component = set()\n",
    "        dfs(links_by_nodes, component, all_component_nodes, node)\n",
    "        connected_components.append(component)\n",
    "    return connected_components\n",
    "\n",
    "def dfs(links_by_nodes, component, all_component_nodes, node):\n",
    "    component.add(node)\n",
    "    all_component_nodes.add(node)\n",
    "\n",
    "    linked_nodes = links_by_nodes.get(node)\n",
    "    if linked_nodes is None:\n",
    "        return\n",
    "    for linked_node in linked_nodes:\n",
    "        if linked_node in all_component_nodes:\n",
    "            continue\n",
    "        dfs(links_by_nodes, component, all_component_nodes, linked_node)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4211a-5c32-48fa-9c3e-67f380047143",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829788e-5655-488f-b157-7ed9cc83f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_dict = {}\n",
    "for k,v in edge_dict.items():\n",
    "    links_by_nodes = {}\n",
    "    for x, y in edge_dict[k]:\n",
    "        if x in links_by_nodes:\n",
    "            links_by_nodes[x].append(y)\n",
    "        else:\n",
    "            links_by_nodes[x] = [y]\n",
    "    instance_dict[k]=find_connected_components(vertex_dict[k], links_by_nodes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b5ad22-62ca-49a1-83b2-bc4e58ceb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_list = []\n",
    "for semantic_key,instance_index_lists  in instance_dict.items():\n",
    "    for instance_indexes in instance_index_lists:            \n",
    "        temp_list=[]\n",
    "        the_dict = {}\n",
    "        for idx in instance_indexes:            \n",
    "            temp_list.append(pths[idx])\n",
    "        the_dict[semantic_key] = temp_list\n",
    "        the_list.append(the_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d1ffcf-2c03-4eb4-ae40-4592cd5ff9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundingbox(xmin,xmax,ymin,ymax):\n",
    "    spath=svg_path(Line(start=(xmin+ymin), end=(xmax+ymin)),\n",
    "             Line(start=(xmax+ymin), end=(xmax+ymax)),\n",
    "             Line(start=(xmax+ymax), end=(xmin+ymax)),\n",
    "             Line(start=(xmin+ymax), end=(xmin+ymin)))\n",
    "    return spath\n",
    "# def min_max_bbox(instance_paths):    \n",
    "#     real_list = []\n",
    "#     imag_list = []\n",
    "#     for path in instance_paths:\n",
    "#         # print(path)\n",
    "#         if isinstance(path[0],Line):\n",
    "#             real_list.extend((path[0].start.real,path[0].end.real))\n",
    "#             imag_list.extend((path[0].start.imag,path[0].end.imag))\n",
    "#         elif isinstance(path[0],Arc):\n",
    "#             if path[0].start.real < path[0].end.real:\n",
    "#                 center_real = (path[0].start.real + path[0].radius.real) \n",
    "#                 center_imag = path[0].start.imag                \n",
    "#                 bottom_real = center_real\n",
    "#                 bottom_imag = center_imag - path[0].radius.imag\n",
    "#                 #print(f'bottom: {center_imag,bottom_imag}')\n",
    "#                 real_list.extend((path[0].start.real, path[0].end.real, bottom_real))\n",
    "#                 imag_list.extend((path[0].start.imag, path[0].end.imag, bottom_imag))\n",
    "#             if path[0].start.real > path[0].end.real:\n",
    "#                 center_real = (path[0].start.real - path[0].radius.real)  \n",
    "#                 center_imag = path[0].start.imag\n",
    "#                 top_real = center_real \n",
    "#                 top_imag = center_imag + path[0].radius.imag\n",
    "#                 #print(f'top: {center_imag,bottom_imag}')\n",
    "#                 real_list.extend((path[0].start.real,path[0].end.real, top_real))\n",
    "#                 imag_list.extend((path[0].start.imag,path[0].end.imag, top_imag))\n",
    "def min_max_bbox(instance_paths):    \n",
    "    real_list = []\n",
    "    imag_list = []\n",
    "    for path in instance_paths:  \n",
    "        \n",
    "        real_list.extend((path[0].start.real,path[0].end.real,path[0].point(0.5).real))\n",
    "        imag_list.extend((path[0].start.imag,path[0].end.imag,path[0].point(0.5).imag))\n",
    "         \n",
    "    xmin = min(real_list)\n",
    "    xmax = max(real_list)\n",
    "    ymin = min(imag_list)*1j\n",
    "    ymax = max(imag_list)*1j\n",
    "    return xmin,xmax,ymin,ymax\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5cbc7a-bb05-4ae4-a007-cd5f3beee986",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_borders =[]\n",
    "instance_text = []\n",
    "# this is where the text goes\n",
    "instance_path = []\n",
    "for i in the_list:\n",
    "    if  list(i.items())[0][0] != '1':\n",
    "        ins_lines = list(i.items())[0][1]\n",
    "        ins_name = list(i.items())[0][0]\n",
    "        mmx= min_max_bbox(ins_lines)\n",
    "        bbx_borders = boundingbox(*mmx)    \n",
    "        instance_borders.append(bbx_borders)\n",
    "        instance_text.append(ins_name)\n",
    "        instance_path.append(bbx_borders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122159ff-7d17-431c-80c6-72d9bed55855",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_list = list()\n",
    "for i in instance_text:\n",
    "    tmp_list.append(num_to_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77881d5d-b38c-4e08-b120-e25bb47f4519",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_text=tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07401a9b-3378-4b61-b288-6754de6183a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "border_atts = []\n",
    "for i in instance_borders:\n",
    "    border_atts.append({'stroke': 'rgb(255,0,0)','fill': 'none','stroke-width': '0.2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7abc25-de7b-4182-b15c-e65da33f1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsvg(pths+instance_borders,attributes= atts+border_atts,font_size=2, text=instance_text, text_path=instance_path, filename='S____100.svg') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cf64c9-df54-4656-a3a0-072c0237cde2",
   "metadata": {},
   "source": [
    "if semantic segmentation underperforms then it means :\n",
    "- if it skips some connection that is necessary. it is destructive because  it could lead us to find many more instances than actual.(false negatives)\n",
    "- if it makes false positives for a class I think it is less destructive because we can filter them out during instance extraction\n",
    "if instance extraction underperforms then it means:\n",
    "- if it creates false positives then it leads to find much less instances\n",
    "- if it creates false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b28ab0-ddb5-4077-97c6-bc433b90eef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1417f-d8ba-49ba-965a-fc49bc475a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
